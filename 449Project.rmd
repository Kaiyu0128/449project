---
title: "Logistic Regression Project: Heart Disease"
author: "William Lin, Ishani Parekh, Kaiyu Yokoi"
output:
  word_document: default
  html_document: default
date: "2023-05-18"
---
 
### Problem statement and description of data
...

### Loading dataset
```{r}
rm(list=ls()) # Clears objects from workspace
#Load the data set 
#heart_disease <-read.csv("/Users/ishani/Desktop/449project/heart_2020_cleaned.csv", stringsAsFactors=TRUE) 
heart_disease <- read.csv("/Users/linw/Desktop/449project/heart_2020_cleaned.csv", stringsAsFactors = TRUE)

head(heart_disease) 

#structure and summary of the data 
str(heart_disease) 
summary(heart_disease)
attach(heart_disease)
```

### Data cleaning

```{r}
### Setting reference variables, ordering levels of ordinal variables
heart_disease$Race = relevel(heart_disease$Race, ref = "White")
heart_disease$GenHealth = factor(heart_disease$GenHealth, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))

### Creating new quantitative variables for ordinal variables using scores
# AgeScore = c(21,27,32,37,42,47,52,57,62,67,72,77,90)
# DiabeticScore = c(0, 1, 2, 3) # (No, No borderline, Yes, Yes pregnant) 
# GenHeatlhScore = c(0, 1, 2, 3, 4) # (Excellent, Very good, Good, Fair, Poor)
heart_disease$Age=rep(0,319795)
  heart_disease$Age[heart_disease$AgeCategory=="18-24"]=21
  heart_disease$Age[heart_disease$AgeCategory=="25-29"]=27
  heart_disease$Age[heart_disease$AgeCategory=="30-34"]=32
  heart_disease$Age[heart_disease$AgeCategory=="35-39"]=37
  heart_disease$Age[heart_disease$AgeCategory=="40-44"]=42
  heart_disease$Age[heart_disease$AgeCategory=="45-49"]=47
  heart_disease$Age[heart_disease$AgeCategory=="50-54"]=52
  heart_disease$Age[heart_disease$AgeCategory=="55-59"]=57
  heart_disease$Age[heart_disease$AgeCategory=="60-64"]=62
  heart_disease$Age[heart_disease$AgeCategory=="65-69"]=67
  heart_disease$Age[heart_disease$AgeCategory=="70-74"]=72
  heart_disease$Age[heart_disease$AgeCategory=="75-79"]=77
  heart_disease$Age[heart_disease$AgeCategory=="80 or older"]=87
  
heart_disease$DiabeticScore=rep(0,319795)
  heart_disease$DiabeticScore[heart_disease$Diabetic=="No"]=0
  heart_disease$DiabeticScore[heart_disease$Diabetic=="No, borderline diabetes"]=1
  heart_disease$DiabeticScore[heart_disease$Diabetic=="Yes"]=2
  heart_disease$DiabeticScore[heart_disease$Diabetic=="Yes (during pregnancy)"]=3
  
heart_disease$GenHealthScore=rep(0,319795)
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Excellent"]=0
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Very good"]=1
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Good"]=2
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Fair"]=3
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Poor"]=4

originalvars = data.frame(heart_disease$AgeCategory, heart_disease$Diabetic,
                          heart_disease$GenHealth)  
  
# Remove ordinal variables; replaced with scores
heart_disease = subset(heart_disease, select = -c(AgeCategory, Diabetic, GenHealth))
```

### Checking proportions in response variable
```{r}
summary(heart_disease$HeartDisease)
count.yes = nrow(heart_disease[heart_disease$HeartDisease == 'Yes', ])
count.no = nrow(heart_disease[heart_disease$HeartDisease == 'No', ])
# Proportion of yes
(prop.yes = count.yes / (count.yes + count.no))
```

The data is unbalanced; only 8.5% of the response variable are Yes's. Training data is likely to be biased towards the No responses. For this project we will downsample the No class such that the proportion of No responses is 2/3. 

```{r}
# Downsampling majority class
set.seed(123)
yes.df <- heart_disease[which(heart_disease$HeartDisease == "Yes"), ]
no.df <- heart_disease[which(heart_disease$HeartDisease == "No"), ]
no.index <- as.numeric(rownames(no.df))
no.sample <- sample(no.index, 2*count.yes)
no.sample.index <- data.frame(no.sample)
no.sample.index <- no.sample.index[order(no.sample), ]
no.downsample <- heart_disease[no.sample.index, ] 
heart_disease2 <- rbind(yes.df, no.downsample)

summary(heart_disease2)
```

### Training and test sets
```{r}
library(ggplot2)
set.seed(1)

train = sample(1:nrow(heart_disease2), 0.8*nrow(heart_disease2)) # 80/20 split
test = (-train)
y.test = heart_disease2$HeartDisease[test]

# Dataframes
train.df = data.frame(heart_disease2[train,])
test.df = data.frame(heart_disease2[test,])
```

### Null model
```{r}
glm.null = glm(HeartDisease~1, data=heart_disease2, subset=train,
               family=binomial)
```

### Fit a logistic regression model with all predictors
```{r}
# AgeCategory, Diabetic, GenHealth replaced with scores
glm.fit=glm(HeartDisease ~ ., data=heart_disease2, subset = train,
            family=binomial)
summary(glm.fit) 
```

We see that PhysicalActivity is not statistically significant at the 0.05 level with an associated p-value of 0.0551. Thus, we will fit a model with PhysicalActivity dropped.

### Select the best subset of variables. Perform a diagnostic on the best model. Perform all possible inferences you can think about.

```{r}
# Model 2: p-value selection
# Remove non-significant variable: PhysicalActivity
glm.fit2 = update(glm.fit, ~ . -PhysicalActivity) 
summary(glm.fit2)

## Checking model goodness-of-fit
# drop1(glm.fit, test="LRT")
anova(glm.fit2, glm.fit, test="LRT") # likelihood-ratio test comparing models
```

We fail to reject H0, so it is safe to assume that the models are the same. Now we use backward subset selection to find the best selection of variables and compare with the previous model. 

```{r}
# Backward subset selection
library(MASS)
stepAIC(glm.fit) # stepwise backward selection using AIC

sapply(list(glm.fit, glm.fit2), AIC) # compare AIC of models
```

After applying backward subset selection, we find that no variables were dropped from the full model containing 17 predictors. Then we compare model 1 and 2 and select the model with the lower AIC. The AICs associated with the all-predictors model and model 2 are 145546 and 145547. However, since the difference in AIC 2, we will choose the simpler model: the model with PhysicalActivity dropped.

```{r}
# Model diagnostic checking
## Checking correlation between predictors
cor(heart_disease[,c("BMI", "PhysicalHealth", "MentalHealth", "SleepTime", "Age","DiabeticScore", "GenHealthScore")])  
```

There is no apparent multicollinearity in the predictors; each correlation coefficient is less than 0.75.

```{r}
## Check for influential values
plot(glm.fit2, which = 4, id.n = 3)

library(dplyr)
library(broom)
# Extract model results
model.data <- augment(glm.fit2) %>% 
  mutate(index = 1:n()) 
# Top 3 influential values
model.data %>% top_n(3, .cooksd)

ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = HeartDisease), alpha = .5) +
  theme_bw()

# Filter potential influential data points with abs(.std.res) > 3
model.data %>% 
  filter(abs(.std.resid) > 3)
```

There are 8 observations that are very influential in the logistic regression model.

```{r}
## Check the linear relationship between 
## continuous predictor variables and the logit of the outcome 

plot(glm.fit2)
```

There is an S-shaped pattern in the deviance residuals plot. This suggests a violation of the linearity assumption between the predictors and the logit of the response. 

```{r}
# Conducting Inference

# Wald test
summary(glm.fit2) 

# Likelihood ratio test
library(car)
Anova(glm.fit2)
```

```{r}
# Confidence Intervals

# Wald confidence intervals for the multiplicative effect on odds
exp(confint.default(glm.fit2)) 
```

### Use the new model to make predictions. 
```{r}
# Making predictions
contrasts(heart_disease2$HeartDisease)
glm.probs=predict(glm.fit2, test.df, type="response")
predicted.classes <- ifelse(glm.probs > 0.5, "Yes", "No")

# Assessing model accuracy
(mean(predicted.classes == test.df$HeartDisease))
```

### Use different pi_0 as a cut-off point and create a confusion table.
```{r}
# pi_0 = 0.2
glm.pred=rep("No", nrow(test.df)) # generate a vector with each element as "No", name this vector glm.pred
glm.pred[glm.probs>0.2]="Yes" #if the predicted probability of heart disease > 0.5, assign predicted direction as "up"
table(glm.pred,test.df$HeartDisease) # produce the confusion matrix
mean(glm.pred==test.df$HeartDisease)

# pi_0 = 0.3
glm.pred=rep("No", nrow(test.df)) # generate a vector with each element as "No", name this vector glm.pred
glm.pred[glm.probs>0.3]="Yes" #if the predicted probability of heart disease > 0.5, assign predicted direction as "up"
table(glm.pred,test.df$HeartDisease) # produce the confusion matrix
mean(glm.pred==test.df$HeartDisease)

# pi_0 = 0.4
glm.pred=rep("No", nrow(test.df)) # generate a vector with each element as "No", name this vector glm.pred
glm.pred[glm.probs>0.4]="Yes" #if the predicted probability of heart disease > 0.5, assign predicted direction as "up"
table(glm.pred,test.df$HeartDisease) # produce the confusion matrix
mean(glm.pred==test.df$HeartDisease)
```

### Perform visualization of data and models.  
```{r}
library(ggplot2)
library(broom)

library(lessR)
#Visual representation of proportion tables
par(mfrow=c(2,2))

PieChart(HeartDisease, hole = 0, values = "%", data = heart_disease,
         fill = c("lightblue", "darkblue"), main = "Heart Disease")
PieChart(Smoking, hole = 0, values = "%", data = heart_disease,
         fill = c("lightblue", "darkblue"), main = "Smoking")
PieChart(AlcoholDrinking, hole = 0, values = "%", data = heart_disease,
         fill = c("lightblue", "darkblue"), main = "Alcohol Drinking")
PieChart(Stroke, hole = 0, values = "%", data = heart_disease,
         fill = c("lightblue", "darkblue"), main = "Stroke")
PieChart(DiffWalking, hole = 0, values = "%", data = heart_disease,
         fill = c("lightblue", "darkblue"), main = "DiffWalking")
PieChart(Sex, hole = 0, values = "%", data = heart_disease,
         fill = c("lightblue", "darkblue"), main = "Sex")
PieChart(Race, hole = 0, values = "%", data = heart_disease,
         fill = c("red", "orange", "yellow", "green", "blue", "purple"), main = "Race")
PieChart(PhysicalActivity, hole = 0, values = "%", data = heart_disease,
         fill = c("lightblue", "blue"), main = "PhysicalActivity")
PieChart(PhysicalActivity, hole = 0, values = "%", data = heart_disease,
         fill = c("lightblue", "blue"), main = "Asthma")
PieChart(KidneyDisease, hole = 0, values = "%", data = heart_disease,
         fill = c("lightblue", "blue"), main = "KidneyDisease")
PieChart(SkinCancer, hole = 0, values = "%", data = heart_disease,
         fill = c("lightblue", "blue"), main = "SkinCancer")
plot(originalvars$heart_disease.AgeCategory)
PieChart(heart_disease.Diabetic, hole = 0, values = "%", data = originalvars,
         fill = c("lightblue", "blue", "darkblue", "purple"), main = "Diabetic")
PieChart(heart_disease.GenHealth, hole = 0, values = "%", data = originalvars,
         fill = c("darkgreen", "green", "lightgreen", "yellow", "red"), main = "GenHealth")
```

```{r}
par(mfrow=c(2,2))
hist(heart_disease$BMI, col = "blue")
hist(heart_disease$PhysicalHealth, col = "blue")
hist(heart_disease$MentalHealth, col = "blue")
hist(heart_disease$SleepTime, col = "blue")
plot(originalvars$heart_disease.AgeCategory, col = "blue", main="Age Category")
```

### Plot the ROC curve, find AUC, and the best cutoff point for classification.
```{r}
library(Epi)
# ROC curve and AUC
hd.roc = ROC(form=HeartDisease ~ . -PhysicalActivity, data=test.df, plot="ROC", MX = TRUE, MI=FALSE)
```

The best cutoff point for classification is 0.337.

```{r}
# pi_0 = 0.337
glm.pred=rep("No", nrow(test.df)) # generate a vector with each element as "No", name this vector glm.pred
glm.pred[glm.probs>0.337]="Yes" #if the predicted probability of heart disease > 0.5, assign predicted direction as "up"
table(glm.pred,test.df$HeartDisease) # produce the confusion matrix
table(glm.pred,test.df$HeartDisease) %>% prop.table()
mean(glm.pred==test.df$HeartDisease)
```

Sensitivity is the true positive rate; P(predicted=1 | y=1) = 4198/(4198+1234) =  0.7728. Specificity is the true negative rate; P(predicted=0 | y=0) = 8226/(8226+2766) = 0.7484.

#Perform LOOCV and k-fold cross-validation.

```{r}
library(boot)
set.seed(1)
# k-fold cross validation
glm.cv <- glm(HeartDisease ~ .-PhysicalActivity, data=train.df, family = binomial)
cv.error.10 <- cv.glm(train.df, glm.cv, K = 10)$delta[1]
cv.error.10
```

The training error is 0.15 and our test error is around 0.25. The difference is around 10%. This suggests our model is overfit to the training data.

#Try the probit link and the identity links to model data.

```{r}
# Probit
glm.probit <- glm(HeartDisease ~ .-PhysicalActivity, data = train.df, family = binomial(link = "probit"))
summary(glm.probit)
```

```{r}
# Recode HeartDisease in training data into 0s and 1s
new_train <- train.df
new_test <- test.df
new_train$HeartDisease <- ifelse(new_train$HeartDisease == "Yes", 1, 0)
new_test$HeartDisease <- ifelse(new_test$HeartDisease == "Yes", 1, 0)

# To get starting value we use the linear fit (least square method) 
mod.ident1=lm(HeartDisease~. -PhysicalActivity,data = new_train)
(g=summary(mod.ident1))

# We use a starting values from least square estimation
strt=c(coef(mod.ident1)[1], coef(mod.ident1)[2], coef(mod.ident1)[3], coef(mod.ident1)[4], coef(mod.ident1)[5], coef(mod.ident1)[6], coef(mod.ident1)[7], coef(mod.ident1)[8], coef(mod.ident1)[9], coef(mod.ident1)[10], coef(mod.ident1)[11], coef(mod.ident1)[12], coef(mod.ident1)[13], coef(mod.ident1)[14],coef(mod.ident1)[15], coef(mod.ident1)[16], coef(mod.ident1)[17], coef(mod.ident1)[18], coef(mod.ident1)[19], coef(mod.ident1)[20], coef(mod.ident1)[21])

mod.ident=glm(HeartDisease ~ . -PhysicalActivity, data = new_train, start=strt, family = gaussian(link = "identity"))
(g1=summary(mod.ident))
```

#Which model works better for this data?

```{r}
# Predictions using identity link
ident.probs <- predict(mod.ident, newdata = new_test)

ident.pred=rep(0, nrow(new_test))
ident.pred[ident.probs>0.337] <- 1 
table(ident.pred,new_test$HeartDisease) # produce the confusion matrix
mean(ident.pred==new_test$HeartDisease)
```

```{r}
# Predictions using probit link
probit.probs <- predict(glm.probit, newdata = test.df)

probit.pred=rep("No", nrow(test.df))
probit.pred[probit.probs>0.337] <- "Yes" 
table(probit.pred,test.df$HeartDisease) # produce the confusion matrix
mean(probit.pred==test.df$HeartDisease)
```

The logit model accuracy is 0.756454, the identity link model accuracy is 0.7363005, and the probit model accuracy is 0.7529226. 

#Write a report













