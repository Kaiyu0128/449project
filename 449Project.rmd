
### Group project

### State the problem and describe the data set


```{r}
library(car)
```

### Loading Dataset
```{r}
rm(list=ls()) # Clears objects from workspace
#Load the data set 
#heart_disease <-read.csv("/Users/ishani/Desktop/449project/heart_2020_cleaned.csv", stringsAsFactors=TRUE) 
#heart_disease <- read.csv("/Users/linw/Desktop/449project/heart_2020_cleaned.csv", stringsAsFactors = TRUE)
heart_disease <- read.csv("/Users/kaiyuyokoi/Desktop/math_449/449project/heart_2020_cleaned.csv", stringsAsFactors = TRUE)

head(heart_disease) 

#structure and summary of the data 
str(heart_disease) 
summary(heart_disease)
attach(heart_disease)
```

### Data cleaning

```{r}
### Setting reference variables, ordering levels of ordinal variables
heart_disease$Race = relevel(heart_disease$Race, ref = "White")
heart_disease$GenHealth = factor(heart_disease$GenHealth, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))

### Creating new quantitative variables for ordinal variables using scores
# AgeScore = c(21,27,32,37,42,47,52,57,62,67,72,77,90)
# DiabeticScore = c(0, 1, 2, 3) # (No, No borderline, Yes, Yes pregnant) 
# GenHeatlhScore = c(0, 1, 2, 3, 4) # (Excellent, Very good, Good, Fair, Poor)
heart_disease$Age=rep(0,319795)
  heart_disease$Age[heart_disease$AgeCategory=="18-24"]=21
  heart_disease$Age[heart_disease$AgeCategory=="25-29"]=27
  heart_disease$Age[heart_disease$AgeCategory=="30-34"]=32
  heart_disease$Age[heart_disease$AgeCategory=="35-39"]=37
  heart_disease$Age[heart_disease$AgeCategory=="40-44"]=42
  heart_disease$Age[heart_disease$AgeCategory=="45-49"]=47
  heart_disease$Age[heart_disease$AgeCategory=="50-54"]=52
  heart_disease$Age[heart_disease$AgeCategory=="55-59"]=57
  heart_disease$Age[heart_disease$AgeCategory=="60-64"]=62
  heart_disease$Age[heart_disease$AgeCategory=="65-69"]=67
  heart_disease$Age[heart_disease$AgeCategory=="70-74"]=72
  heart_disease$Age[heart_disease$AgeCategory=="75-79"]=77
  heart_disease$Age[heart_disease$AgeCategory=="80 or older"]=87
  
heart_disease$DiabeticScore=rep(0,319795)
  heart_disease$DiabeticScore[heart_disease$Diabetic=="No"]=0
  heart_disease$DiabeticScore[heart_disease$Diabetic=="No, borderline diabetes"]=1
  heart_disease$DiabeticScore[heart_disease$Diabetic=="Yes"]=2
  heart_disease$DiabeticScore[heart_disease$Diabetic=="Yes (during pregnancy)"]=3
  
heart_disease$GenHealthScore=rep(0,319795)
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Excellent"]=0
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Very good"]=1
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Good"]=2
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Fair"]=3
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Poor"]=4

# Remove ordinal variables; replaced with scores
heart_disease = subset(heart_disease, select = -c(AgeCategory, Diabetic, GenHealth))
```

```{r}
# Null model
glm.null = glm(HeartDisease~1,data=heart_disease,family=binomial)
```

### Fit a logistic regression model with all predictors
```{r}
# AgeCategory, Diabetic, GenHealth replaced with scores
glm.fit=glm(HeartDisease ~ ., data=heart_disease, family=binomial)
summary(glm.fit) 
```

We see that PhysicalActivity is not statistically significant at the 0.05 level with an associated p-value of 0.08527. Thus, we will fit a model with PhysicalActivity dropped.

### Select the best subset of variables. Perform a diagnostic on the best model. Perform all possible inferences you can think about.

```{r}
# Model 2: p-value selection
# Remove non-significant variable: PhysicalActivity
glm.fit2 = update(glm.fit, ~ . -PhysicalActivity) 
summary(glm.fit2)

## Checking model goodness-of-fit
# drop1(glm.fit, test="LRT")
anova(glm.fit2, glm.fit, test="LRT") # likelihood-ratio test comparing models
```

We now use backward subset selection to find the best selection of variables and compare with the previous model. 

```{r}
# Backward subset selection
library(MASS)
#stepAIC(glm.fit) # stepwise backward selection using AIC

sapply(list(glm.fit, glm.fit2), AIC) # compare AIC of models
```

After applying backward subset selection, we find that no variables were dropped from the full model containing 17 predictors. Then we compare model 1 and 2 and select the model with the lower AIC. The AICs associated with the all-predictors model and model 2 are 145546 and 145547. However, since the difference in AIC is less than 2, we will choose the simpler model: the model with PhysicalActivity dropped.

```{r}
# Model diagnostic checking
## Checking correlation between predictors
cor(heart_disease[,c("BMI", "PhysicalHealth", "MentalHealth", "SleepTime", "Age","DiabeticScore", "GenHealthScore")])  
```

```{r}
# Conducting Inference
summary(glm.fit2) # Wald z-scores
```

```{r}
# Confidence Intervals

# Wald confidence interval
confint.default(glm.fit2) 
# Wald confidence interval for the effect
exp(confint.default(glm.fit2)) 
```

```{r}
###########################################################

#Likelihood ratio test
#Model comparison, compare models g(mu)=alpha+beta x  and g(mu)=alpha

# -2 * (L0-L1) , in your summary 
#Null deviance=2(L0-Lsaturated), residual deviance=devuiance= -2*(L1-Lsaturated)
#therefore -2 * (L0-L1)=nul.deviance-deviance

LLstat=(g$null.deviance-g$deviance)
p_val=1-pchisq(LLstat, 1)
p_val


# automated way to perform the likelihood ratio test, Likelihood-ratio test of snoring and heart disease (comparison with the null
drop1(fit, test="Chisq")

#Or can use Anova, but need to install many packages for that
#Anova(fit) 

###########################################################
#Deviance, goodness of fit

#Deviance
Deviance=deviance(fit)
Deviance
# or
Deviance=g$deviance
Deviance

#Goodness of Fit
P_value=1-pchisq(g$deviance, g$df.res)
P_value

################################################################

```

### Use the new model to make predictions. 
```{r}
# Making predictions
contrasts(heart_disease$HeartDisease)
glm.probs=predict(glm.fit2, type="response")
predicted.classes <- ifelse(glm.probs > 0.5, "Yes", "No")
head(predicted.classes)

# Assessing model accuracy
mean(predicted.classes == heart_disease$HeartDisease)
```

### Use different pi_0 as a cut-off point and create a confusion table.
```{r}
# pi_0 = 0.2
glm.pred=rep("No", nrow(heart_disease)) # generate a vector with each element as "No", name this vector glm.pred
glm.pred[glm.probs>0.2]="Yes" #if the predicted probability of heart disease > 0.5, assign predicted direction as "up"
table(glm.pred,heart_disease$HeartDisease) # produce the confusion matrix
mean(glm.pred==heart_disease$HeartDisease)

# pi_0 = 0.3
glm.pred=rep("No", nrow(heart_disease)) # generate a vector with each element as "No", name this vector glm.pred
glm.pred[glm.probs>0.3]="Yes" #if the predicted probability of heart disease > 0.5, assign predicted direction as "up"
table(glm.pred,heart_disease$HeartDisease) # produce the confusion matrix
mean(glm.pred==heart_disease$HeartDisease)

# pi_0 = 0.4
glm.pred=rep("No", nrow(heart_disease)) # generate a vector with each element as "No", name this vector glm.pred
glm.pred[glm.probs>0.4]="Yes" #if the predicted probability of heart disease > 0.5, assign predicted direction as "up"
table(glm.pred,heart_disease$HeartDisease) # produce the confusion matrix
mean(glm.pred==heart_disease$HeartDisease)
```

### Perform visualization of data and models.  
```{r}
library(ggplot2)
ggplot(data.frame(Probability = glm.probs), aes(x=Probability)) +
  geom_histogram(bins=30, fill="skyblue", color="black") +
  labs(x="Predicted Probability", y="Count") +
  theme_minimal()
#plot()
#pairs()
```

### Plot the ROC curve, find AUC, and the best cutoff point for classification.
```{r}
library(Epi)
# ROC curve and AUC
hd.roc = ROC(form=HeartDisease~. -PhysicalActivity, data=heart_disease, plot="ROC")

#Sensitivity is the true positive rate; P(predicted=1 | y=1) = 51/(51+46) = 0.53. #Specificity is the true negative rate; P(predicted=0 | y=0) = 633/(633+320) = 0.66.
```


#Perform LOOCV and k-fold cross-validation.

```{r}
#LOOCV - problem 7 ch 5 hw 448
#Logistic_crabs_all - k-fold

#install.packages("DAAG")
library(DAAG)
cv.binary(glm1.fit)


glm.fit3=glm(HeartDisease ~ Age + BMI, data = heart_disease[-1, ], family=binomial)
summary(glm.fit3)

predict.glm(glm.fit3, heart_disease[1, ], type = "response") > 0.5


err <- numeric(nrow(heart_disease))

for (i in 1:nrow(heart_disease)) {
  train <- heart_disease[-i, ]
  #i
  glm.fit4 <- glm(HeartDisease ~ Age + BMI, data = train, family = binomial)
  #ii
  post <- predict(glm.fit4, heart_disease[i, ], type = "response")
  #iii
  pred <- ifelse(post > 0.5, "Yes", "No")
  #iv
  er <- as.integer(pred != heart_disease$HeartDisease[i])
  err[i] <- er
}


err

numOne <- sum(err == 1)

numOne


mean(err)



```




#Try the probit link and the identity links to model data.

```{r}

Malformation.ident=glm(cbind(Present, Absent)~alcohol, family=binomial(link="identity"), data=Malform )
summary(Malformation.ident)


fit1 <- glm(evolved ~ ideology, family=binomial(link="identity"), data=Evo)
summary(fit, dispersion=1)
summary(fit1)


Heart$x <- recode(Heart$snoring, never = 0, occasional = 2, nearly_every_night = 4, every_night = 5)
fit <- glm(yes/(yes+no) ~ x, family=binomial(link=probit),
           + weights=yes+no, data=Heart)
summary(fit)

```


#Which model works better for this data?






#Write a report













