---
title: "Logistic Regression Project: Heart Disease"
author: "William Lin, Ishani Parekh, Kaiyu Yokoi"
output:
  word_document: default
  html_document: default
date: "2023-05-18"
---

### Problem statement and description of data
...

### Loading dataset
```{r}
rm(list=ls()) # Clears objects from workspace
#Load the data set 
#heart_disease <-read.csv("/Users/ishani/Desktop/449project/heart_2020_cleaned.csv", stringsAsFactors=TRUE) 
heart_disease <- read.csv("/Users/linw/Desktop/449project/heart_2020_cleaned.csv", stringsAsFactors = TRUE)

head(heart_disease) 

#structure and summary of the data 
str(heart_disease) 
summary(heart_disease)
attach(heart_disease)
```

### Data cleaning

```{r}
### Setting reference variables, ordering levels of ordinal variables
heart_disease$Race = relevel(heart_disease$Race, ref = "White")
heart_disease$GenHealth = factor(heart_disease$GenHealth, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"))

### Creating new quantitative variables for ordinal variables using scores
# AgeScore = c(21,27,32,37,42,47,52,57,62,67,72,77,90)
# DiabeticScore = c(0, 1, 2, 3) # (No, No borderline, Yes, Yes pregnant) 
# GenHeatlhScore = c(0, 1, 2, 3, 4) # (Excellent, Very good, Good, Fair, Poor)
heart_disease$Age=rep(0,319795)
  heart_disease$Age[heart_disease$AgeCategory=="18-24"]=21
  heart_disease$Age[heart_disease$AgeCategory=="25-29"]=27
  heart_disease$Age[heart_disease$AgeCategory=="30-34"]=32
  heart_disease$Age[heart_disease$AgeCategory=="35-39"]=37
  heart_disease$Age[heart_disease$AgeCategory=="40-44"]=42
  heart_disease$Age[heart_disease$AgeCategory=="45-49"]=47
  heart_disease$Age[heart_disease$AgeCategory=="50-54"]=52
  heart_disease$Age[heart_disease$AgeCategory=="55-59"]=57
  heart_disease$Age[heart_disease$AgeCategory=="60-64"]=62
  heart_disease$Age[heart_disease$AgeCategory=="65-69"]=67
  heart_disease$Age[heart_disease$AgeCategory=="70-74"]=72
  heart_disease$Age[heart_disease$AgeCategory=="75-79"]=77
  heart_disease$Age[heart_disease$AgeCategory=="80 or older"]=87
  
heart_disease$DiabeticScore=rep(0,319795)
  heart_disease$DiabeticScore[heart_disease$Diabetic=="No"]=0
  heart_disease$DiabeticScore[heart_disease$Diabetic=="No, borderline diabetes"]=1
  heart_disease$DiabeticScore[heart_disease$Diabetic=="Yes"]=2
  heart_disease$DiabeticScore[heart_disease$Diabetic=="Yes (during pregnancy)"]=3
  
heart_disease$GenHealthScore=rep(0,319795)
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Excellent"]=0
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Very good"]=1
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Good"]=2
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Fair"]=3
  heart_disease$GenHealthScore[heart_disease$GenHealth=="Poor"]=4

# Remove ordinal variables; replaced with scores
heart_disease = subset(heart_disease, select = -c(AgeCategory, Diabetic, GenHealth))
```

### Visualization of response variable
```{r}
summary(heart_disease$HeartDisease)
count.yes = nrow(heart_disease[heart_disease$HeartDisease == 'Yes', ])
count.no = nrow(heart_disease[heart_disease$HeartDisease == 'No', ])
# Proportion of yes
(prop.yes = count.yes / (count.yes + count.no))
```

The data is unbalanced; only 8.5% of the response variable are Yes's. Training data is likely to be biased towards the No responses. For this project we will downsample the No class such that the proportion of No responses is 2/3. 

```{r}
# Downsampling majority class
set.seed(123)
yes.df <- heart_disease[which(heart_disease$HeartDisease == "Yes"), ]
no.df <- heart_disease[which(heart_disease$HeartDisease == "No"), ]
no.index <- as.numeric(rownames(no.df))
no.sample <- sample(no.index, 2*count.yes)
no.sample.index <- data.frame(no.sample)
no.sample.index <- no.sample.index[order(no.sample), ]
no.downsample <- heart_disease[no.sample.index, ] 
heart_disease2 <- rbind(yes.df, no.downsample)

summary(heart_disease2)
```

### Training and test sets
```{r}
library(ggplot2)
set.seed(1)

train = sample(1:nrow(heart_disease2), 0.8*nrow(heart_disease2)) # 80/20 split
test = (-train)
y.test = heart_disease2$HeartDisease[test]

# Dataframes
train.df = data.frame(heart_disease2[train,])
test.df = data.frame(heart_disease2[test,])
```

### Null model
```{r}
glm.null = glm(HeartDisease~1, data=heart_disease2, subset=train,
               family=binomial)
```

### Fit a logistic regression model with all predictors
```{r}
# AgeCategory, Diabetic, GenHealth replaced with scores
glm.fit=glm(HeartDisease ~ ., data=heart_disease2, subset = train,
            family=binomial)
summary(glm.fit) 
```

We see that PhysicalActivity is not statistically significant at the 0.05 level with an associated p-value of 0.0551. Thus, we will fit a model with PhysicalActivity dropped.

### Select the best subset of variables. Perform a diagnostic on the best model. Perform all possible inferences you can think about.

```{r}
# Model 2: p-value selection
# Remove non-significant variable: PhysicalActivity
glm.fit2 = update(glm.fit, ~ . -PhysicalActivity) 
summary(glm.fit2)

## Checking model goodness-of-fit
# drop1(glm.fit, test="LRT")
anova(glm.fit2, glm.fit, test="LRT") # likelihood-ratio test comparing models
```

We fail to reject H0, so it is safe to assume that the models are the same. Now we use backward subset selection to find the best selection of variables and compare with the previous model. 

```{r}
# Backward subset selection
library(MASS)
stepAIC(glm.fit) # stepwise backward selection using AIC

sapply(list(glm.fit, glm.fit2), AIC) # compare AIC of models
```

After applying backward subset selection, we find that no variables were dropped from the full model containing 17 predictors. Then we compare model 1 and 2 and select the model with the lower AIC. The AICs associated with the all-predictors model and model 2 are 145546 and 145547. However, since the difference in AIC 2, we will choose the simpler model: the model with PhysicalActivity dropped.

```{r}
# Model diagnostic checking
## Checking correlation between predictors
cor(heart_disease[,c("BMI", "PhysicalHealth", "MentalHealth", "SleepTime", "Age","DiabeticScore", "GenHealthScore")])  
```

There is no apparent multicollinearity in the predictors; each correlation coefficient is less than 0.75.

```{r}
## Check the linear relationship between 
## continuous predictor variables and the logit of the outcome 
library(dplyr)
library(tidyr)

glm.probs=predict(glm.fit2, type="response") 

# # Select only numeric predictors
# num_only <- train.df %>%
#   dplyr::select_if(is.numeric) 
# predictors <- colnames(num_only)
# # Bind the logit and tidying the data for plot
# num_only <- num_only %>%
#   mutate(logit = log(glm.probs/(1-glm.probs))) %>%
#   gather(key = "predictors", value = "predictor.value", -logit)

```

```{r}
# Conducting Inference

# Wald test
summary(glm.fit2) 

# Likelihood ratio test
library(car)
Anova(glm.fit2)
```

```{r}
# Confidence Intervals

# Wald confidence intervals for the multiplicative effect on odds
exp(confint.default(glm.fit2)) 
```

### Use the new model to make predictions. 
```{r}
# Making predictions
contrasts(heart_disease2$HeartDisease)
glm.probs=predict(glm.fit2, type="response")
predicted.classes <- ifelse(glm.probs > 0.5, "Yes", "No")

# Assessing model accuracy
(mean(predicted.classes == heart_disease2$HeartDisease))
```

### Use different pi_0 as a cut-off point and create a confusion table.
```{r}
# pi_0 = 0.2
glm.pred=rep("No", nrow(heart_disease2)) # generate a vector with each element as "No", name this vector glm.pred
glm.pred[glm.probs>0.2]="Yes" #if the predicted probability of heart disease > 0.5, assign predicted direction as "up"
table(glm.pred,heart_disease2$HeartDisease) # produce the confusion matrix
mean(glm.pred==heart_disease2$HeartDisease)

# pi_0 = 0.3
glm.pred=rep("No", nrow(heart_disease2)) # generate a vector with each element as "No", name this vector glm.pred
glm.pred[glm.probs>0.3]="Yes" #if the predicted probability of heart disease > 0.5, assign predicted direction as "up"
table(glm.pred,heart_disease2$HeartDisease) # produce the confusion matrix
mean(glm.pred==heart_disease2$HeartDisease)

# pi_0 = 0.4
glm.pred=rep("No", nrow(heart_disease2)) # generate a vector with each element as "No", name this vector glm.pred
glm.pred[glm.probs>0.4]="Yes" #if the predicted probability of heart disease > 0.5, assign predicted direction as "up"
table(glm.pred,heart_disease2$HeartDisease) # produce the confusion matrix
mean(glm.pred==heart_disease2$HeartDisease)
```

### Perform visualization of data and models.  
```{r}
library(ggplot2)
library(broom)

par(mfrow = c(2, 2))

plot(heart_disease2$HeartDisease, col = c("lightblue", "firebrick"))
ggplot(glm(HeartDisease ~ BMI, family = binomial, data=heart_disease2))
plot(jitter(HeartDisease, 0.08) ~ , data=heart_disease2)

```

### Plot the ROC curve, find AUC, and the best cutoff point for classification.
```{r}
library(Epi)
# ROC curve and AUC
hd.roc = ROC(form=HeartDisease~. -PhysicalActivity, data=heart_disease2[train, ], plot="ROC")

#Sensitivity is the true positive rate; P(predicted=1 | y=1) = 51/(51+46) = 0.53. #Specificity is the true negative rate; P(predicted=0 | y=0) = 633/(633+320) = 0.66.
```

#Perform LOOCV and k-fold cross-validation.

```{r}
#LOOCV - problem 7 ch 5 hw 448
#Logistic_crabs_all - k-fold

library(DAAG)
cv.binary(glm1.fit)


glm.fit3=glm(HeartDisease ~ Age + BMI, data = heart_disease[-1, ], family=binomial)
summary(glm.fit3)

predict.glm(glm.fit3, heart_disease[1, ], type = "response") > 0.5


err <- numeric(nrow(heart_disease))

for (i in 1:nrow(heart_disease)) {
  train <- heart_disease[-i, ]
  #i
  glm.fit4 <- glm(HeartDisease ~ Age + BMI, data = train, family = binomial)
  #ii
  post <- predict(glm.fit4, heart_disease[i, ], type = "response")
  #iii
  pred <- ifelse(post > 0.5, "Yes", "No")
  #iv
  er <- as.integer(pred != heart_disease$HeartDisease[i])
  err[i] <- er
}


err

numOne <- sum(err == 1)

numOne


mean(err)



```




#Try the probit link and the identity links to model data.

```{r}

Malformation.ident=glm(cbind(Present, Absent)~alcohol, family=binomial(link="identity"), data=Malform )
summary(Malformation.ident)


fit1 <- glm(evolved ~ ideology, family=binomial(link="identity"), data=Evo)
summary(fit, dispersion=1)
summary(fit1)


Heart$x <- recode(Heart$snoring, never = 0, occasional = 2, nearly_every_night = 4, every_night = 5)
fit <- glm(yes/(yes+no) ~ x, family=binomial(link=probit),
           + weights=yes+no, data=Heart)
summary(fit)

```


#Which model works better for this data?






#Write a report













